{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e66001-fc7e-4c7e-b26a-4ffb0bd46115",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install stop-words\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import swifter\n",
    "import string\n",
    "import csv\n",
    "from stop_words import get_stop_words\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78374c87-8426-46db-9a1a-29b5567ab793",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "file_path = 'translated_output_complete.csv'\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', nrows=200000)\n",
    "    print(\"Data loaded successfully with UTF-8 encoding.\")\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='latin1', nrows=200000)\n",
    "        print(\"Data loaded successfully with Latin-1 encoding.\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='iso-8859-1', nrows=200000)\n",
    "        print(\"Data loaded successfully with ISO-8859-1 encoding.\")\n",
    "\n",
    "def clean_text(text, lang):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+|\\@\\w+|\\#\", '', text, flags=re.MULTILINE)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    if lang == 'hebrew':\n",
    "        stop_words = set(stopwords.words('hebrew'))\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "        \n",
    "    else:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f1108-abbb-4cf6-a7cd-fb89d7343baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying text preprocessing...\")\n",
    "# df['original_text'] = df.apply(lambda row: clean_text(row['original_text'], 'english'), axis=1)\n",
    "df['text'] = df.apply(lambda row: clean_text(row['text'], 'english'), axis=1)\n",
    "df['translated_data'] = df.apply(lambda row: clean_text(row['translated_data'], 'hebrew'), axis=1)\n",
    "# df['translated_text'] = df.apply(lambda row: clean_text(row['translated_text'], 'hebrew'), axis=1)\n",
    "print(\"Text preprocessing completed.\")\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "print(\"Preparing data for training...\")\n",
    "# english_texts = df['original_text'].values\n",
    "# hebrew_texts = df['translated_text'].values\n",
    "english_texts = df['text'].values\n",
    "hebrew_texts = df['translated_data'].values\n",
    "classifications = np.where(df['labels'] == 'suicide', 1, 0)\n",
    "\n",
    "combined_texts = np.concatenate((english_texts, hebrew_texts))\n",
    "combined_labels = np.concatenate((classifications, classifications))\n",
    "print(\"Data preparation completed.\")\n",
    "\n",
    "# Split data\n",
    "print(\"Splitting data into training, validation, and test sets...\")\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(combined_texts, combined_labels, test_size=0.2, random_state=42)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "print(\"Data splitting completed.\")\n",
    "\n",
    "# Tokenization and Encoding\n",
    "print(\"Starting tokenization and encoding...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75bc27-0388-4260-ac80-6076055ed17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(texts, max_length=256):\n",
    "    print(f\"Encoding {len(texts)} texts...\")\n",
    "    encoded_data = tokenizer(text=texts.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    print(\"Encoding completed.\")\n",
    "    return encoded_data\n",
    "\n",
    "print(\"Starting data encoding...\")\n",
    "\n",
    "# Encode training data\n",
    "train_encoded = encode_data(train_texts)\n",
    "print(\"Training data encoded.\")\n",
    "\n",
    "# Encode validation data\n",
    "val_encoded = encode_data(val_texts)\n",
    "print(\"Validation data encoded.\")\n",
    "\n",
    "# Encode test data\n",
    "test_encoded = encode_data(test_texts)\n",
    "print(\"Test data encoded.\")\n",
    "\n",
    "print(\"Creating DataLoaders...\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(train_encoded['input_ids'], train_encoded['attention_mask'], torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(val_encoded['input_ids'], val_encoded['attention_mask'], torch.tensor(val_labels))\n",
    "test_dataset = TensorDataset(test_encoded['input_ids'], test_encoded['attention_mask'], torch.tensor(test_labels))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "print(\"Training DataLoader created.\")\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "print(\"Validation DataLoader created.\")  # Fixed the missing quotation mark\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "print(\"Test DataLoader created.\")\n",
    "\n",
    "print(\"All DataLoaders are ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb033cb-6d2d-4445-a309-a4be6c6876ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EnhancedSuicideClassifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        super(EnhancedSuicideClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "        # Freeze BERT layers\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "\n",
    "        # Bi-directional LSTM\n",
    "        self.bilstm = nn.LSTM(768, 512, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.fc7 = nn.Linear(64, 1)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.bn6 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        attn_output, _ = self.multihead_attn(sequence_output.transpose(0, 1), sequence_output.transpose(0, 1), sequence_output.transpose(0, 1))\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "        # Bi-directional LSTM\n",
    "        lstm_output, _ = self.bilstm(attn_output)\n",
    "        lstm_output = lstm_output[:, -1, :]  # Take the last hidden state\n",
    "\n",
    "        # Forward pass through fully connected layers with batch normalization\n",
    "        x = self.fc1(lstm_output)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        output = self.fc7(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnhancedSuicideClassifier(dropout_rate=0.5).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "criterion_main = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 30\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion_main):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\")\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        main_output = model(input_ids, attention_mask)\n",
    "        loss = criterion_main(main_output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    main_predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            main_output = model(input_ids, attention_mask)\n",
    "            main_preds = torch.sigmoid(main_output.squeeze()) > 0.5\n",
    "            main_predictions.extend(main_preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return (accuracy_score(true_labels, main_predictions),\n",
    "            precision_score(true_labels, main_predictions),\n",
    "            recall_score(true_labels, main_predictions),\n",
    "            f1_score(true_labels, main_predictions))\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_f1 = 0\n",
    "no_improve = 0\n",
    "\n",
    "\n",
    "patience = 7\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, criterion_main)\n",
    "    val_acc, val_prec, val_rec, val_f1 = evaluate(model, val_dataloader)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}, Precision: {val_prec:.4f}, Recall: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        no_improve = 0\n",
    "        # print(\"New best model saved!\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve == patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    print() \n",
    "# model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n",
    "\n",
    "test_acc, test_prec, test_rec, test_f1 = evaluate(model, test_dataloader)\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb913bd-1827-48dc-a1c1-cb245eb398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_model(model, test_loader, criterion):\n",
    "#     model.eval()\n",
    "#     total_test_loss = 0.0\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             input_ids, attention_mask, labels = batch\n",
    "#             input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(input_ids, attention_mask)\n",
    "#             loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "#             total_test_loss += loss.item()\n",
    "\n",
    "#             predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "#             predictions = np.where(predictions > 0.5, 1, 0)\n",
    "#             all_predictions.extend(predictions.flatten())\n",
    "#             all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "#     avg_test_loss = total_test_loss / len(test_loader)\n",
    "#     print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "#     accuracy = accuracy_score(all_labels, all_predictions)\n",
    "#     precision = precision_score(all_labels, all_predictions)\n",
    "#     recall = recall_score(all_labels, all_predictions)\n",
    "#     f1 = f1_score(all_labels, all_predictions)\n",
    "#     auc = roc_auc_score(all_labels, all_predictions)\n",
    "#     conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "#     print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, AUC: {auc:.4f}')\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(conf_matrix)\n",
    "\n",
    "# # Load the best model and evaluate on the test set\n",
    "# model.load_state_dict(torch.load('200k_Remove_stopWords.pth'))\n",
    "# criterion = nn.BCEWithLogitsLoss()  # Initialize the criterion without any arguments\n",
    "\n",
    "# # Test the model\n",
    "# test_model(model, test_dataloader, criterion)\n",
    "\n",
    "\n",
    "# # # Load the best model and evaluate on the test set\n",
    "# # model.load_state_dict(torch.load('200k_Remove_stopWords.pth'))\n",
    "# # test_model(model, test_dataloader, nn.BCEWithLogitsLoss('200k_Remove_stopWords.pth'))\n",
    "\n",
    "# # # Function to predict custom strings\n",
    "# # def predict_custom_string(model, tokenizer, text, lang='english'):\n",
    "# #     cleaned_text = clean_text(text, lang)\n",
    "# #     # print(cleaned_text)\n",
    "# #     encoded_input = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "# #     input_ids = encoded_input['input_ids'].to(device)\n",
    "# #     attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "# #     model.eval()\n",
    "# #     with torch.no_grad():\n",
    "# #         output = model(input_ids, attention_mask)\n",
    "\n",
    "# #     prediction = torch.sigmoid(output).cpu().numpy()\n",
    "# #     return prediction[0][0]\n",
    "\n",
    "# # # Custom Predictions\n",
    "# # custom_texts = [\n",
    "# #     \"אשתי עזבה החיים בזבל לא בא לי לחיות\",\n",
    "# #     \"אני רוצה להתאבד\",\n",
    "# #     \"i wanna live forever\",\n",
    "# #     \"אני אוהב את החיים האלה\",\n",
    "# #     \"אני שונא את האדם הזה\",\n",
    "# #     \"אני לא יכול לשאת את זה יותר אני אובדני\",\n",
    "# #     \"התאבדות\",\n",
    "# #     \"אושר והנאה ושמחה מהחיים\",\n",
    "# #     \"want kill myself\",\n",
    "# #     \"really good\",\n",
    "# #     \"שמחה ואושר\",\n",
    "# #     \"i love you\",\n",
    "# #     \"i hate you and i dont like my life\",\n",
    "# #     \"i want to commit suicide\",\n",
    "# #     \"אני רוצה לבתאבד אבל אני אוהב את המשפחה שלי אז אני בוודאות לא אתאבד\",\n",
    "# #     \"איזה כיף יש מבצע בזארה\",\n",
    "# #     \"i am tired of losing in call of duty\",\n",
    "# #     \"בזמן האחרון אני קצת מוטרד מהמצב וחושב עלייך די הרבה השיחות שלנו כמו שירים בהופעות אני זוכר את הכל בעל פה\",\n",
    "# #     \"זה אני האחד שסגד ליופיך שנתן את הכל בשבילך ששברת את ליבו בלכתך, זה אני שהיית ונשארת בדמי\"\n",
    "# # ]\n",
    "\n",
    "# # for text in custom_texts:\n",
    "# #     lang = 'hebrew' if re.search(r'[א-ת]', text) else 'english'\n",
    "# #     prediction = predict_custom_string(model, tokenizer, text, lang)\n",
    "# #     print(f'Prediction for \"{text}\": {prediction:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d77bf-4abe-4fda-a1d4-a296cc0fbbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530a9ee-a8c7-4098-bb7c-8b99d9e7b09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
